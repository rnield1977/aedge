Week 2 Notes
--------------------------------------------------------------------------------------
Out of sample SSE
SSE <- sum((PointsPrediction - NBA$PTS)^2)
SST <- sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 <- 1 - SSE/SST
RMSE <- sqrt(SSE/nrow(NBA_test))

Relative Error
(Observed ILI - Estimated ILI)/Observed ILI

Use step() to find model

Week 3 Notes
--------------------------------------------------------------------------------------

Sensitivity
True Positives / True Positives + False Negatives

Specificity
True Negatives / True Negatives + False Positives

 TN FP
 FN TP

False Negative Error Rate = FN / TP+ FN
False Positive Error Rate = FP / TN + FP

To get the confusion Matrix 
table(qualityTrain$PoorCare, predictTrain > 0.5) 

    FALSE TRUE
  0    70    4 
  1    15   10
  
 
  
  Compute Sensitivity (Measure Classify Possitive Correctly)
  10/25
  
  Compute Specificity (Measure Classify Negative Correctly)
  70/74
  
Creating ROCR Example:
library(ROCR)
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
ROCRperf = performance(ROCRpred,"tpr","fpr")
plot(ROCRperf, colorize="TRUE")
or 
plot(ROCRperf, colorize="TRUE", print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

#### FOR TREES
spamRFPred <- predict(spamRF, newdata=train, type="prob") for RandomForrest
no class for CART 
USE SECOND PARAMETER


Area Under Curve
auc = as.numeric(performance(ROCRpred, "auc")@y.values)
    
    
#Using the Mice Package
library(mice)
imputed = complete(mice(simple))


#Better way of filtering based on columns
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
songsTrain = songsTrain[ , !(names(songsTrain) %in% nonvars) ]
OR
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)

use response on predictions

Get the Odds Ratios for each coef
exp(coef(mylogit))
Can do.
exp(cbind(OR = coef(mylogit), confint(mylogit)))

Steps for the odds ratio:
1- Take coef of the model -4.2411574 + 0.3869904*male + 0.8867192*race - 0.0001756*age + 0.4433007*state2 + 0.8349797*state3 - 3.3967878*state4 - 0.1238867*time.served + 0.0802954*max.sentence + 1.6119919*multiple.offenses + 0.6837143*crime2 - 0.2781054*crime3 - 0.0117627*crime4 =  -1.700629

2- Odds Ratio is OR <- exp(-1.700629)
3- Prediction probability
  1/(1+exp(1.700629))
  
  


Example 2 of Mice
vars.for.imputation = setdiff(names(loans), "not.fully.paid")
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed



Set Profit under Logistic regression probabilities.
> test$profit = exp(test$int.rate*3) - 1
> test$profit[test$not.fully.paid == 1] = -1


Example of sorting via a cutoff
cutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100]

Week 4

Cart Example
library(rpart)
library(rpart.plot)
StevensTree <- rpart(Reverse ~ Circuit + Issue + Petitioner + respondent + LowerCourt + Unconst, data=train, method="class", minbucket=25)

prp(StevensTree)
PredictCart = predict(StevensTree, newdata=test, type="class")
table(test$Reverse, PredictCart)

PredictROC <- predict(StevensTree, newdata=test)
pred <- prediction(PredictROC[,2], test$Reverse)
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=TRUE)

as.numeric(performance(pred, "auc")@y.values)

Random Forrest
library(randomForest)
StevensForrest <- randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=train, nodesize=25, ntree=200)
> PredictForest <- predict(StevensForest, newdata=test)
PredictForest <- predict(StevensForrest, newdata=test)


k-Fold Cross- Validation
cp is like R2 and AIC smaller is better
library(caret)
library(e1071)
numFolds <- trainControl(method="cv", number=10)
cpGrid <- expand.grid(.cp=seq(0.01,0.5,0.01))
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)

StevensTreeCV <- rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=train, method="class", cp=0.18 )

PredictCV <- predict(StevensTreeCV, newdata=test, type="class")
table(test$Reverse, PredictCV)

# To get precentage of of particular variable
Example:
table(Claims$bucket2009) / nrow(Claims)

#Penalty Matrix Example

PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)

as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)) * PenaltyMatrix

sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)) * PenaltyMatrix)/nrow(ClaimsTest)

#Example
We then want to get a higher then
(110138 + 10721 + 2774 + 1539 + 104 )/ nrow(ClaimsTest)
and lower then
sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)) * PenaltyMatrix)/nrow(ClaimsTest)

--------------
Standard Basline
NOTE this is the first column of the matrix.
PM <- c(0,2,4,6,8)
table(ClaimsTest$bucket2009) * PM
sum(table(ClaimsTest$bucket2009) * PM) / nrow(ClaimsTest)


---------------
Example of plotting fitted value as well as adding $ to pch


---------------

Example of plot of final model using caret
best.tree <- tr$finalModel
prp(best.tree)

---------------
Example of logistic regression interaction
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")


-----------------
When taking ROCR for CART be sure to remove type="class" when predicting


--------------
One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split. To view this metric, run the following lines of R code (replace "MODEL" with the name of your random forest model):

vu = varUsed(MODEL, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(MODEL$forest$xlevels[vusorted$ix]))


Show model impurity
A different metric we can look at is related to "impurity", which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. To compute this metric, run the following command in R (replace "MODEL" with the name of your random forest model):
varImpPlot(MODEL)


-----------------------

Week 5 

Bag of Words

Always do string as factor is false

library(tm)
library(SnowballC)

corpus <- Corpus(VectorSource(tweets$Tweet))
corpus[[1]]
corpus <- tm_map(corpus,tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus,removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)


frequencies <- DocumentTermMatrix(corpus)

or dtm <- DocumentTermMatrix(corpus)

frequencies

# Look at matrix 

inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)

sparse = removeSparseTerms(frequencies, 0.995)
sparse

tweetsSparse = as.data.frame(as.matrix(sparse))

colnames(tweetsSparse) = make.names(colnames(tweetsSparse))

tweetsSparse$Negative = tweets$Negative

set.seed(123)

split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)

trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)

########
String Wrap
strwrap(emails$email[1])


#######
Examples of Row Sums
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))



######
training set predictions
tree <- rpart(trial~., data=train, method="class",)
# Note, no type given
trainp <- predict(tree)


################
Week 6

With Hierarchical clustering you would prefer to choose the cluster with the most distance between the splits
** To remove unneeded columns set that column to NULL
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL

# Remove duplicates
movies = unique(movies)

Hierarchical clustering
distances <- dist(movies[2:20], method="euclidean")
clusterMovies <- hclust(distances, method = "ward") 
plot(clusterMovies)

clusterGroups = cutree(clusterMovies, k=10)

tapply(movies$Action, clusterGroups,mean)

### Now see where a paticular row number falls

subset(movies, Title=="Men in Black (1997)")
Gives us 257
clusterGroups[257]

### Now grab just the cluster
cluster2 <- subset(movies, clusterGroups ==2)

### View the cluster
cluster2[1:10,]$Title


### Clustering with Matrix
flowerMatrix <- as.matrix(flower)
flowerVector <- as.vector(flowerMatrix)


rect.hclust(clusterIntensity, k=3, border="red")
flowerClusters <- cutree(clusterIntensity, k=3)

See how the clustering is broken up.
tapply(flowerVector,flowerClusters,mean)

#view the cluster
dim(flowerClusters) = c(50,50)
image(flowerClusters, axes=FALSE)

# View image in greyscale.
image(flowerMatrix, axes=FALSE, col = grey(seq(0,1,length=256)))  


Dist issues.
Error: cannot allocate vector of size 498.0 Gb
n=365636
n*(n-1)/2

#kmeans example
k = 5
KMC <- kmeans(healthyVector,centers=k, iter.max=1000)
str(KMC)
healthyClusters <- KMC$cluster
dim(healthyClusters) <- c(nrow(healthyMatrix),ncol(healthyMatrix))
image(healthyClusters, axes=FALSE, col=rainbow(k))


## Predicting
library(flexclust)
KMC.kcca <- as as.kcca(KMC,healthyVector)
tumorClusters <- predict(KMC.kcca, newdata=tumorVector)
dim(tumorClusters) <- c(nrow(tumorMatrix), ncol(tumorMatrix))

##Predicting Stocks
  #Get rid of the dependants
  limitedTrain$PositiveDec = NULL
  limitedTest$PositiveDec = NULL
  library(caret)
  #Normalize
  preproc = preProcess(limitedTrain)
  normTrain = predict(preproc, limitedTrain)
  normTest = predict(preproc, limitedTest)
  #Cluster
  km <- kmeans(normTrain, centers=3)
  library(flexclust)
  km.kcca = as.kcca(km, normTrain)
  clusterTrain = predict(km.kcca)
  clusterTest = predict(km.kcca, newdata=normTest)
  stocksTrain1 <- subset(stocksTrain, clusterTest == 1)
  stocksTrain2 <- subset(stocksTrain, clusterTest == 2)
  stocksTrain3 <- subset(stocksTrain, clusterTest == 3)
  stocksTest1 <- subset(stocksTest, clusterTest == 1)
  stocksTest2 <- subset(stocksTest, clusterTest == 2)
  stocksTest3 <- subset(stocksTest, clusterTest == 3)
  
  #Compute Cluster Specific 
  AllPredictions = c(PredictTest1, PredictTest2, PredictTest3)
  AllOutcomes = c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec)

##### Get means of a col
tail(sort(colMeans(dkc7)))



##### Normalize a dataframe
library(caret)
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)