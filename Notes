Week 2 Notes
--------------------------------------------------------------------------------------
Out of sample SSE
SSE <- sum((PointsPrediction - NBA$PTS)^2)
SST <- sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 <- 1 - SSE/SST
RMSE <- sqrt(SSE/nrow(NBA_test))

Relative Error
(Observed ILI - Estimated ILI)/Observed ILI

Use step() to find model

Week 3 Notes
--------------------------------------------------------------------------------------

Sensitivity
True Positives / True Positives + False Negatives

Specificity
True Negatives / True Negatives + False Positives

 TN FP
 FN TP

False Negative Error Rate = FN / TP+ FN
False Positive Error Rate = FP / TN + FP

To get the confusion Matrix 
table(qualityTrain$PoorCare, predictTrain > 0.5) 

    FALSE TRUE
  0    70    4 
  1    15   10
  
 
  
  Compute Sensitivity (Measure Classify Possitive Correctly)
  10/25
  
  Compute Specificity (Measure Classify Negative Correctly)
  70/74
  
Creating ROCR Example:
library(ROCR)
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
ROCRperf = performance(ROCRpred,"tpr","fpr")
plot(ROCRperf, colorize="TRUE")
or 
plot(ROCRperf, colorize="TRUE", print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

#### FOR TREES
spamRFPred <- predict(spamRF, newdata=train, type="prob") for RandomForrest
no class for CART 
USE SECOND PARAMETER


Area Under Curve
auc = as.numeric(performance(ROCRpred, "auc")@y.values)
    
    
#Using the Mice Package
library(mice)
imputed = complete(mice(simple))


#Better way of filtering based on columns
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
songsTrain = songsTrain[ , !(names(songsTrain) %in% nonvars) ]
OR
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)

use response on predictions

Get the Odds Ratios for each coef
exp(coef(mylogit))
Can do.
exp(cbind(OR = coef(mylogit), confint(mylogit)))

Steps for the odds ratio:
1- Take coef of the model -4.2411574 + 0.3869904*male + 0.8867192*race - 0.0001756*age + 0.4433007*state2 + 0.8349797*state3 - 3.3967878*state4 - 0.1238867*time.served + 0.0802954*max.sentence + 1.6119919*multiple.offenses + 0.6837143*crime2 - 0.2781054*crime3 - 0.0117627*crime4 =  -1.700629

2- Odds Ratio is OR <- exp(-1.700629)
3- Prediction probability
  1/(1+exp(1.700629))
  
  


Example 2 of Mice
vars.for.imputation = setdiff(names(loans), "not.fully.paid")
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed



Set Profit under Logistic regression probabilities.
> test$profit = exp(test$int.rate*3) - 1
> test$profit[test$not.fully.paid == 1] = -1


Example of sorting via a cutoff
cutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100]

Week 4

Cart Example
library(rpart)
library(rpart.plot)
StevensTree <- rpart(Reverse ~ Circuit + Issue + Petitioner + respondent + LowerCourt + Unconst, data=train, method="class", minbucket=25)

prp(StevensTree)
PredictCart = predict(StevensTree, newdata=test, type="class")
table(test$Reverse, PredictCart)

PredictROC <- predict(StevensTree, newdata=test)
pred <- prediction(PredictROC[,2], test$Reverse)
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=TRUE)

as.numeric(performance(pred, "auc")@y.values)

Random Forrest
library(randomForest)
StevensForrest <- randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=train, nodesize=25, ntree=200)
> PredictForest <- predict(StevensForest, newdata=test)
PredictForest <- predict(StevensForrest, newdata=test)


k-Fold Cross- Validation
cp is like R2 and AIC smaller is better
library(caret)
library(e1071)
numFolds <- trainControl(method="cv", number=10)
cpGrid <- expand.grid(.cp=seq(0.01,0.5,0.01))
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)

StevensTreeCV <- rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data=train, method="class", cp=0.18 )

PredictCV <- predict(StevensTreeCV, newdata=test, type="class")
table(test$Reverse, PredictCV)

# To get precentage of of particular variable
Example:
table(Claims$bucket2009) / nrow(Claims)

#Penalty Matrix Example

PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)

as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)) * PenaltyMatrix

sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)) * PenaltyMatrix)/nrow(ClaimsTest)

#Example
We then want to get a higher then
(110138 + 10721 + 2774 + 1539 + 104 )/ nrow(ClaimsTest)
and lower then
sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)) * PenaltyMatrix)/nrow(ClaimsTest)

--------------
Standard Basline
NOTE this is the first column of the matrix.
PM <- c(0,2,4,6,8)
table(ClaimsTest$bucket2009) * PM
sum(table(ClaimsTest$bucket2009) * PM) / nrow(ClaimsTest)


---------------
Example of plotting fitted value as well as adding $ to pch


---------------

Example of plot of final model using caret
best.tree <- tr$finalModel
prp(best.tree)

---------------
Example of logistic regression interaction
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")


-----------------
When taking ROCR for CART be sure to remove type="class" when predicting


--------------
One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split. To view this metric, run the following lines of R code (replace "MODEL" with the name of your random forest model):

vu = varUsed(MODEL, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(MODEL$forest$xlevels[vusorted$ix]))


Show model impurity
A different metric we can look at is related to "impurity", which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. To compute this metric, run the following command in R (replace "MODEL" with the name of your random forest model):
varImpPlot(MODEL)


-----------------------

Week 5 

Bag of Words

Always do string as factor is false

library(tm)
library(SnowballC)

corpus <- Corpus(VectorSource(tweets$Tweet))
corpus[[1]]
corpus <- tm_map(corpus,tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus,removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)


frequencies <- DocumentTermMatrix(corpus)

or dtm <- DocumentTermMatrix(corpus)

frequencies

# Look at matrix 

inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)

sparse = removeSparseTerms(frequencies, 0.995)
sparse

tweetsSparse = as.data.frame(as.matrix(sparse))

colnames(tweetsSparse) = make.names(colnames(tweetsSparse))

tweetsSparse$Negative = tweets$Negative

set.seed(123)

split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)

trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)

########
String Wrap
strwrap(emails$email[1])


#######
Examples of Row Sums
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))



######
training set predictions
tree <- rpart(trial~., data=train, method="class",)
# Note, no type given
trainp <- predict(tree)


################
Week 6

With Hierarchical clustering you would prefer to choose the cluster with the most distance between the splits
** To remove unneeded columns set that column to NULL
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL

# Remove duplicates
movies = unique(movies)

Hierarchical clustering
distances <- dist(movies[2:20], method="euclidean")
clusterMovies <- hclust(distances, method = "ward.D") 
plot(clusterMovies)

clusterGroups = cutree(clusterMovies, k=10)

tapply(movies$Action, clusterGroups,mean)

### Now see where a paticular row number falls

subset(movies, Title=="Men in Black (1997)")
Gives us 257
clusterGroups[257]

### Now grab just the cluster
cluster2 <- subset(movies, clusterGroups ==2)

### View the cluster
cluster2[1:10,]$Title


### Clustering with Matrix
flowerMatrix <- as.matrix(flower)
flowerVector <- as.vector(flowerMatrix)


rect.hclust(clusterIntensity, k=3, border="red")
flowerClusters <- cutree(clusterIntensity, k=3)

See how the clustering is broken up.
tapply(flowerVector,flowerClusters,mean)

#view the cluster
dim(flowerClusters) = c(50,50)
image(flowerClusters, axes=FALSE)

# View image in greyscale.
image(flowerMatrix, axes=FALSE, col = grey(seq(0,1,length=256)))  


Dist issues.
Error: cannot allocate vector of size 498.0 Gb
n=365636
n*(n-1)/2

#kmeans example
k = 5
KMC <- kmeans(healthyVector,centers=k, iter.max=1000)
str(KMC)
healthyClusters <- KMC$cluster
dim(healthyClusters) <- c(nrow(healthyMatrix),ncol(healthyMatrix))
image(healthyClusters, axes=FALSE, col=rainbow(k))


## Predicting
library(flexclust)
KMC.kcca <- as as.kcca(KMC,healthyVector)
tumorClusters <- predict(KMC.kcca, newdata=tumorVector)
dim(tumorClusters) <- c(nrow(tumorMatrix), ncol(tumorMatrix))

##Predicting Stocks
  #Get rid of the dependants
  limitedTrain$PositiveDec = NULL
  limitedTest$PositiveDec = NULL
  library(caret)
  #Normalize
  preproc = preProcess(limitedTrain)
  normTrain = predict(preproc, limitedTrain)
  normTest = predict(preproc, limitedTest)
  #Cluster
  km <- kmeans(normTrain, centers=3)
  library(flexclust)
  km.kcca = as.kcca(km, normTrain)
  clusterTrain = predict(km.kcca)
  clusterTest = predict(km.kcca, newdata=normTest)
  stocksTrain1 <- subset(stocksTrain, clusterTest == 1)
  stocksTrain2 <- subset(stocksTrain, clusterTest == 2)
  stocksTrain3 <- subset(stocksTrain, clusterTest == 3)
  stocksTest1 <- subset(stocksTest, clusterTest == 1)
  stocksTest2 <- subset(stocksTest, clusterTest == 2)
  stocksTest3 <- subset(stocksTest, clusterTest == 3)
  
  #Compute Cluster Specific 
  AllPredictions = c(PredictTest1, PredictTest2, PredictTest3)
  AllOutcomes = c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec)

##### Get means of a col
tail(sort(colMeans(dkc7)))



##### Normalize a dataframe
library(caret)
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)

#### Kaggle #####
Bag of words combining Testing and Training

CorpusHeadline = Corpus(VectorSource(c(Train$Headline, Test$Headline)))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
dtm = DocumentTermMatrix(CorpusHeadline)
sparse = removeSparseTerms(dtm, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparse))
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
HeadlineWordsTrain = head(HeadlineWords, nrow(NewsTrain))
HeadlineWordsTest = tail(HeadlineWords, nrow(NewsTest))



###Week 7
ggplot(WHO, aes(x = FertilityRate, y = Under15)) + geom_point()

geom_line()
geom_point(color = "blue", size = 3, shape = 17)
ggtitle("Fertility Rate vs. Gross National Income")
stat_smooth(method = "lm")
#Change error level
stat_smooth(method = "lm", level = 0.99)
#turn off
stat_smooth(method = "lm", se = FALSE)
#Make Orange.
stat_smooth(method = "lm", colour = "orange")

#Date Conersion example
mvt$Date <- strptime(mvt$Date, format="%m/%d/%y %H:%M")
#Get Hour from the date
mvt$Date$hour

#Group all data in one line
ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1))

#ordered Factor

WeekdayCounts$Var1 <- factor(WeekdayCounts$Var1, ordered=true, levels=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))

ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1)) + xlab("Day of week") + ylab("Crime")

#Example of adding color
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1, color=Var1, size=2)) 

#Adding a heatmap
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1, color=Var1, size=2)) 
#2
ggplot(DayHourCounts, aes(x=Hour, y=Var1)) + geom_tile(aes(fill=Freq)) + scale_fill_gradient(name="Total MV Thefts") + theme(axis.title.y = element_blank()

#Change colors

ggplot(DayHourCounts, aes(x=Hour, y=Var1)) + geom_tile(aes(fill=Freq)) + scale_fill_gradient(name="Total MV Thefts", low="white", high="red") + theme(axis.title.y = element_blank())


### Mapping geo
library(maps)
library(ggmap)
chicago <- get_map(location="chicago", zoom=11)
ggmap(chicago)

###

Adding point to map.
ggmap(chicago) + geom_point(data=mvt[1:100,], aes(x=Longitude, y=Latitude))

# Plot these points on our map:
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq))

# Change the color scheme:
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq)) + scale_colour_gradient(low="yellow", high="red")

# We can also use the geom_tile geometry
ggmap(chicago) + geom_tile(data = LatLonCounts, aes(x = Long, y = Lat, alpha = Freq), fill="red")

## United States

# Load the map of the US
statesMap = map_data("state")

str(statesMap)

# Plot the map:
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "black") 

## Merge Example

murderMap = merge(statesMap, murders, by="region")

ggplot(murderMap, aes(x = long, y = lat, group = group, fill = Murders)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")

# Plot a map of the population:
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = Population)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")

# Create a new variable that is the number of murders per 100,000 population:
murderMap$MurderRate = murderMap$Murders / murderMap$Population * 100000

# Redo our plot with murder rate:
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = MurderRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")

# Redo the plot, removing any states with murder rates above 10:
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = MurderRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))


### Bar Chart with slanted titles and adjusted vjust
ggplot(intl, aes(x=Region, y=PercentOfIntl)) +
geom_bar(stat="identity", fill="dark blue") +
geom_text(aes(label=PercentOfIntl), vjust=-0.4) +
ylab("Percent of International Students") +
theme(axis.title.x = element_blank(), axis.text.x = element_text(angle = 45, hjust = 1))


## Transform example to reorder desc by PercentofInt
intl = transform(intl, Region = reorder(Region, -PercentOfIntl,mean))


### More Mapping Examples

# Lets "fix" that in the intlall dataset
intlall$Citizenship[intlall$Citizenship=="China (People's Republic Of)"] = "China"

# We'll repeat our merge and order from before
world_map = merge(map_data("world"), intlall, 
                  by.x ="region",
                  by.y = "Citizenship")
world_map = world_map[order(world_map$group, world_map$order),]

ggplot(world_map, aes(x=long, y=lat, group=group)) +
  geom_polygon(aes(fill=Total), color="black") +
  coord_map("mercator")


# We can try other projections - this one is visually interesting
ggplot(world_map, aes(x=long, y=lat, group=group)) +
  geom_polygon(aes(fill=Total), color="black") +
  coord_map("ortho", orientation=c(20, 30, 0))

ggplot(world_map, aes(x=long, y=lat, group=group)) +
  geom_polygon(aes(fill=Total), color="black") +
  coord_map("ortho", orientation=c(-37, 175, 0))


#### Usage of Melt

# First few rows of our melted households dataframe
head(melt(households, id="Year"))

households[,1:3]

melt(households, id="Year")[1:10,3]
melt(households, id="Year")[1:10,]

# Plot it
ggplot(melt(households, id="Year"),       
       aes(x=Year, y=value, color=variable)) +
  geom_line(size=2) + geom_point(size=5) +  
  ylab("Percentage of Households")


# Example of redoing labels and colors
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black") + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")



# Example of node graph
library(igraph)
g = graph.data.frame(edges, FALSE, users)
plot(g, vertex.size=5, vertex.label=NA)

##Check degrees
degree(g)
## Change size based on freq
V(g)$size = degree(g)/2+2
plot(g, vertex.label=NA)

## Change color
V(g)$color = "black"
V(g)$color[V(g)$gender == "A"] = "red"
V(g)$color[V(g)$gender == "B"] = "gray"

# Word Cloud
Create Bag of words but do not stem
install wordcloud
wordcloud(colnames(allTweets), colSums(allTweets))

## Center Cloud
wordcloud(colnames(allTweets), colSums(allTweets), random.order=FALSE)

## Color Brewer 
As the color
brewer.pal()
display.brewer.all()
Example to wordcloud
wordcloud(colnames(allTweets), colSums(allTweets), random.order=FALSE, colors=brewer.pal(9, "Blues")[c(-1, -2, -3, -4)])

